# -*- coding: utf-8 -*-
"""hfprac2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1F05OM6qbYYZP4GzGuc8UOD8rQRcmhlrK

Installing necessary libraries
"""

!pip install datasets transformers[sentencepiece] sacrebleu -q

"""Importing required libraries"""

import os
import sys
import transformers
import tensorflow as tf
from datasets import load_dataset
from transformers import AutoTokenizer
from transformers import TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq
from transformers import AdamWeightDecay
from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM

"""Setting model checkpoint for the Helsinki-NLP English-Hindi translation model"""

model_checkpoint = "Helsinki-NLP/opus-mt-en-hi"

"""Loading the IITB English-Hindi translation dataset"""

raw_datasets = load_dataset("cfilt/iitb-english-hindi")

"""Displaying dataset details"""

print(raw_datasets)

"""Displaying the second training example in the dataset"""

print(raw_datasets['train'][1])

"""Loading the tokenizer associated with the Helsinki-NLP model"""

tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

tokenizer("Hello, this is a sentence!")

tokenizer(["Hello, this is a sentence!", "This is another sentence."])

"""Tokenizing a Hindi sentence using the tokenizer configured for the target language"""

with tokenizer.as_target_tokenizer():
    print(tokenizer(["एक्सेर्साइसर पहुंचनीयता अन्वेषक"]))

# Setting maximum token lengths for input and target sentences
max_input_length = 128
max_target_length = 128
# Specifying source and target languages
source_lang = "en"
target_lang = "hi"


# Defining a preprocessing function
def preprocess_function(examples):
      # Extracting input sentences in the source language
    inputs = [ex[source_lang] for ex in examples["translation"]]
      # Extracting target sentences in the target language
    targets = [ex[target_lang] for ex in examples["translation"]]
     # Tokenizing input sentences
    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)

    # Setup the tokenizer for targets
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(targets, max_length=max_target_length, truncation=True)

    # Adding tokenized target sentences as labels
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

"""Testing the preprocessing function on a small sample"""

preprocess_function(raw_datasets["train"][:2])

"""Applying the preprocessing function to the entire dataset"""

tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)

"""Loading the pre-trained sequence-to-sequence model"""

model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)

"""Setting training parameters"""

batch_size = 16
learning_rate = 2e-5
weight_decay = 0.01
num_train_epochs = 1

"""Creating data collators for training and generation"""

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors="tf")

"""Preparing the tokenized datasets for training and validation"""

generation_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors="tf", pad_to_multiple_of=128)

train_dataset = model.prepare_tf_dataset(
    tokenized_datasets["test"],
    batch_size=batch_size,
    shuffle=True,
    collate_fn=data_collator,
)

validation_dataset = model.prepare_tf_dataset(
    tokenized_datasets["validation"],
    batch_size=batch_size,
    shuffle=False,
    collate_fn=data_collator,
)

generation_dataset = model.prepare_tf_dataset(
    tokenized_datasets["validation"],
    batch_size=8,
    shuffle=False,
    collate_fn=generation_data_collator,
)

"""Compiling the model with the Adam optimizer"""

optimizer = AdamWeightDecay(learning_rate=learning_rate, weight_decay_rate=weight_decay)
model.compile(optimizer=optimizer)

"""Training the model for the specified number of epochs"""

model.fit(train_dataset, validation_data=validation_dataset, epochs=1)

"""Saving the trained model"""

model.save_pretrained("tf_model/")

"""Loading the saved model and tokenizer






"""

tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = TFAutoModelForSeq2SeqLM.from_pretrained("tf_model/")

"""Tokenizing and translating an input English sentence"""

input_text  = "My name Bappy. My youtube channel name is DSwithBappy"

tokenized = tokenizer([input_text], return_tensors='np')
out = model.generate(**tokenized, max_length=128)
print(out)

"""Decoding and printing the translated output in Hindi"""

with tokenizer.as_target_tokenizer():
    print(tokenizer.decode(out[0], skip_special_tokens=True))

"""testing more.."""

input_text  = "i feel sleepy and  just want to leave for home after office"

tokenized = tokenizer([input_text], return_tensors='np')
out = model.generate(**tokenized, max_length=128)
print(out)

with tokenizer.as_target_tokenizer():
    print(tokenizer.decode(out[0], skip_special_tokens=True))

